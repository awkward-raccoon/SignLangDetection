<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
</head>

<body>
    <div>Teachable Machine Image Model</div>
    <button type="button" onclick="init()">Start</button>
    <div id="label-container"></div>
    <div class="container">
     <video class="input_video"
     style="display: None"></video>
     <canvas class="output_canvas" width="1280px" height="720px"></canvas>
  </div>

  <script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

    // the link to your model provided by Teachable Machine export panel
    const URL = "http://127.0.0.1:8080/tm-my-image-model_A-F/";
    const videoElement = document.getElementsByClassName('input_video')[0];
    const canvasElement = document.getElementsByClassName('output_canvas')[0];
    const canvasCtx = canvasElement.getContext('2d');
    let labelContainer = document.getElementById("label-container");


    let model, webcam, maxPredictions;

    // calling holistic api from mediapipe cdn
    const holistic = new Holistic({locateFile: (file) => {
      return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;
    }});
    console.log("Holistic loaded")

      // Load the image model and setup the webcam
    async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // or files from your local hard drive
        // Note: the pose library adds "tmImage" object to your window (window.tmImage)
        model = await tmImage.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();
        console.log("model loaded")
        
        // append elements to the DOM
        // document.getElementById("webcam-container").appendChild(canvasElement);
        // labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }

        // configuring our holistic api according to webcam and requirements
        holistic.setOptions({
          modelComplexity: 1,
          smoothLandmarks: true,
          enableSegmentation: true,
          smoothSegmentation: true,
          refineFaceLandmarks: true,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5
        });
  
        // creating an camera using holistic api 
        const camera = new Camera(videoElement, {
          onFrame: async () => {
            await holistic.send({image: videoElement});
          },
          width: 1280,
          height: 720
        });

        camera.start();
        console.log("Camera element created")

        // on detecting webcam image draw landmarks
        window.requestAnimationFrame(loop);
        // holistic.onResults(draw); //change
      }

    async function loop() {
        // webcam.update(); // update the webcam frame
        holistic.onResults(draw);
        await predict();
        window.requestAnimationFrame(loop);
    }

    // run the webcam image through the image model
    async function predict() {
      // predict can take in an image, video or canvas html element
      const prediction = await model.predict(canvasElement);
      for (let i = 0; i < maxPredictions; i++) {
          const classPrediction =
              prediction[i].className + ": " + prediction[i].probability.toFixed(2);
          labelContainer.childNodes[i].innerHTML = classPrediction;
      }
    }

    function draw(results) {
      console.log("Called draw function")
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    
      // Only overwrite existing pixels.
      // canvasCtx.globalCompositeOperation = 'source-in';
      // canvasCtx.fillStyle = 'white';
      // canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);
    
      // Only overwrite missing pixels.
      canvasCtx.globalCompositeOperation = 'destination-atop';
      canvasCtx.drawImage(
          results.image, 0, 0, canvasElement.width, canvasElement.height);
    
      canvasCtx.globalCompositeOperation = 'source-over';
      //drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS,
        //              {color: '#00FF00', lineWidth: 4});
      //drawLandmarks(canvasCtx, results.poseLandmarks,
      //              {color: '#FF0000', lineWidth: 2});
      //drawConnectors(canvasCtx, results.faceLandmarks, FACEMESH_TESSELATION,
       //               {color: '#C0C0C070', lineWidth: 1});
      drawConnectors(canvasCtx, results.leftHandLandmarks, HAND_CONNECTIONS,
                      {color: '#CC0000', lineWidth: 5});
      drawLandmarks(canvasCtx, results.leftHandLandmarks,
                    {color: '#00FF00', lineWidth: 2});
      drawConnectors(canvasCtx, results.rightHandLandmarks, HAND_CONNECTIONS,
                      {color: '#00CC00', lineWidth: 5});
      drawLandmarks(canvasCtx, results.rightHandLandmarks,
                    {color: '#FF0000', lineWidth: 2});
      canvasCtx.restore();
    }
    
</script>
</body>
</html>
